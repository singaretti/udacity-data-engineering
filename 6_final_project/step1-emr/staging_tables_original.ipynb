{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0db7e8f86455447e933f514dcba59ceb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>0</td><td>application_1610824394546_0001</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-172-31-33-100.us-west-2.compute.internal:20888/proxy/application_1610824394546_0001/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-172-31-34-193.us-west-2.compute.internal:8042/node/containerlogs/container_1610824394546_0001_01_000001/livy\">Link</a></td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import boto3\n",
    "import json\n",
    "import configparser\n",
    "from botocore.exceptions import ClientError\n",
    "import psycopg2\n",
    "from pyspark.sql.functions import concat, col, lit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data about Covid-19 cases to be processed from Germany and Italy\n",
    "#### This s3 bucket should be updated with new cases using API or manual inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### source data #1 - Germany data about COVID-19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3638073192204fbeacd00eb7f4f734c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "germany_data = 's3://udacity-de-capstone-project/germany/covid_de.csv'\n",
    "germany = spark.read.format('csv').option(\"header\", \"true\").load(germany_data)\n",
    "germany = germany.select('state',\n",
    "                         'county',\n",
    "                         'age_group',\n",
    "                         'gender',\n",
    "                         concat(col('date'),lit('T00:00:00')).alias('dt'),\n",
    "                         'cases',\n",
    "                         'deaths',\n",
    "                         'recovered')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d5dcedf51ce40feb51644bf8f58d993",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "germany_staging_table_s3_path = 's3://udacity-de-capstone-project/germany/staging_table'\n",
    "germany.write.format(\"json\").save(germany_staging_table_s3_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### source data #2 - Italy Cities/Provinces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "970203054ee54546b2599321170ee0f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "italy_province_data = 's3://udacity-de-capstone-project/italy/dati-json/dpc-covid19-ita-province.json'\n",
    "italy_province = spark.read.json(italy_province_data,multiLine=True).withColumnRenamed('data','dt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83243632884848f1b91fa08a7f8ceca6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "italy_province_staging_table_s3_path = 's3://udacity-de-capstone-project/italy/staging_table/province'\n",
    "italy_province.write.format(\"json\").save(italy_province_staging_table_s3_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### source data #3 - Italy Regions and COVID-19 data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2a9584dd15a4db2adabc96af5d98ad8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "italy_regioni_data = 's3://udacity-de-capstone-project/italy/dati-json/dpc-covid19-ita-regioni.json'\n",
    "italy_regioni = spark.read.json(italy_regioni_data,multiLine=True).withColumnRenamed('data','dt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21e0650763d94a04a4abc1bc3bd2be33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "italy_regioni_staging_table_s3_path = 's3://udacity-de-capstone-project/italy/staging_table/regioni'\n",
    "italy_regioni.write.format(\"json\").save(italy_regioni_staging_table_s3_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters to create AWS Redshift cluster\n",
    "#### If IAM Role you plan to use already exists, just set this name in specific parameter and it's going to be used instead of recreate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dccbdd732c5e488fb2cd4085a7dfd9de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "AWS_KEY='AKIAV7GIEU7KTFJQ7PEC'\n",
    "AWS_SECRET='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'\n",
    "DWH_CLUSTER_TYPE='multi-node'\n",
    "DWH_NUM_NODES=4\n",
    "DWH_NODE_TYPE='dc2.large'\n",
    "\n",
    "DWH_IAM_ROLE_NAME='RoleForRedshift_FinalProject'\n",
    "DWH_IAM_ROLE_WITH_ARN='arn:aws:iam::410589112277:role/RoleForRedshift_FinalProject'\n",
    "DWH_CLUSTER_IDENTIFIER='covid-19-analysis'\n",
    "DWH_DB='database'\n",
    "DWH_DB_USER='singaretti'\n",
    "DWH_DB_PASSWORD='Singaretti1'\n",
    "DWH_PORT=5439"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2199f73305f4675bc0cbfb95a344eaf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "iam = boto3.client('iam',aws_access_key_id=AWS_KEY,\n",
    "                     aws_secret_access_key=AWS_SECRET,\n",
    "                     region_name='us-west-2'\n",
    "                  )\n",
    "\n",
    "redshift = boto3.client('redshift',\n",
    "                       region_name=\"us-west-2\",\n",
    "                       aws_access_key_id=AWS_KEY,\n",
    "                       aws_secret_access_key=AWS_SECRET\n",
    "                       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a22322dac28641deba9b893c57a2ab22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.1 Creating a new IAM Role\n",
      "An error occurred (EntityAlreadyExists) when calling the CreateRole operation: Role with name RoleForRedshift_FinalProject already exists.\n",
      "1.2 Attaching Policy\n",
      "1.3 Get the IAM role ARN\n",
      "arn:aws:iam::410589112277:role/RoleForRedshift_FinalProject"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    print(\"1.1 Creating a new IAM Role\") \n",
    "    dwhRole = iam.create_role(\n",
    "        Path='/',\n",
    "        RoleName=DWH_IAM_ROLE_NAME,\n",
    "        Description = \"Allows Redshift clusters to call AWS services on your behalf.\",\n",
    "        AssumeRolePolicyDocument=json.dumps(\n",
    "            {'Statement': [{'Action': 'sts:AssumeRole',\n",
    "               'Effect': 'Allow',\n",
    "               'Principal': {'Service': 'redshift.amazonaws.com'}}],\n",
    "             'Version': '2012-10-17'})\n",
    "    )    \n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    \n",
    "    \n",
    "print(\"1.2 Attaching Policy\")\n",
    "\n",
    "iam.attach_role_policy(RoleName=DWH_IAM_ROLE_NAME,\n",
    "                       PolicyArn=\"arn:aws:iam::aws:policy/AmazonS3ReadOnlyAccess\"\n",
    "                      )['ResponseMetadata']['HTTPStatusCode']\n",
    "\n",
    "print(\"1.3 Get the IAM role ARN\")\n",
    "roleArn = iam.get_role(RoleName=DWH_IAM_ROLE_NAME)['Role']['Arn']\n",
    "\n",
    "print(roleArn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91e0266c05564112a305c8ea9ee2c433",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "try:\n",
    "    response = redshift.create_cluster(        \n",
    "        #HW\n",
    "        ClusterType=DWH_CLUSTER_TYPE,\n",
    "        NodeType=DWH_NODE_TYPE,\n",
    "        NumberOfNodes=int(DWH_NUM_NODES),\n",
    "\n",
    "        #Identifiers & Credentials\n",
    "        DBName=DWH_DB,\n",
    "        ClusterIdentifier=DWH_CLUSTER_IDENTIFIER,\n",
    "        MasterUsername=DWH_DB_USER,\n",
    "        MasterUserPassword=DWH_DB_PASSWORD,\n",
    "        \n",
    "        #Roles (for s3 access)\n",
    "        IamRoles=[roleArn]\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e5899e2daac477c8f4d3998c907dab7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ClusterIdentifier-> covid-19-analysis\n",
      "NodeType-> dc2.large\n",
      "ClusterStatus-> available\n",
      "MasterUsername-> singaretti\n",
      "DBName-> database\n",
      "Endpoint-> {'Address': 'covid-19-analysis.coi6foj1tbge.us-west-2.redshift.amazonaws.com', 'Port': 5439}\n",
      "NumberOfNodes-> 4\n",
      "VpcId-> vpc-0e5c237dda5620d8a\n",
      "IamRoles-> [{'IamRoleArn': 'arn:aws:iam::410589112277:role/RoleForRedshift_FinalProject', 'ApplyStatus': 'in-sync'}]"
     ]
    }
   ],
   "source": [
    "all_keys = redshift.describe_clusters(ClusterIdentifier=DWH_CLUSTER_IDENTIFIER)['Clusters'][0]\n",
    "keys_to_show = [\"ClusterIdentifier\", \"NodeType\", \"ClusterStatus\", \"MasterUsername\", \"DBName\", \"Endpoint\", \"NumberOfNodes\", 'VpcId','IamRoles']\n",
    "\n",
    "for key in keys_to_show:\n",
    "  print(f\"{key}-> {all_keys[key]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ecc77151b9344eb38c55eee824dec97c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "conn = psycopg2.connect(host='covid-19-analysis.coi6foj1tbge.us-west-2.redshift.amazonaws.com',\n",
    "                        dbname='database',\n",
    "                        user='singaretti',\n",
    "                        password='Singaretti1',\n",
    "                        port='5439')\n",
    "conn.set_session(autocommit=True)\n",
    "cur = conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eebfd84495564c44b564b78785cfab10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# DROP TABLES\n",
    "staging_italy_province_table_drop = 'DROP TABLE IF EXISTS staging_italy_province;'\n",
    "staging_italy_regioni_table_drop = 'DROP TABLE IF EXISTS staging_italy_regioni;'\n",
    "staging_germany_table_drop = 'DROP TABLE IF EXISTS staging_germany;'\n",
    "\n",
    "# CREATE TABLES\n",
    "staging_italy_province_table_create= (\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS staging_italy_province\n",
    "(\n",
    "codice_provincia INT,\n",
    "codice_regione INT,\n",
    "dt VARCHAR,\n",
    "denominazione_provincia VARCHAR,\n",
    "denominazione_regione VARCHAR,\n",
    "lat FLOAT,\n",
    "long FLOAT,\n",
    "sigla_provincia VARCHAR,\n",
    "stato VARCHAR,\n",
    "totale_casi VARCHAR)\n",
    "\"\"\")\n",
    "\n",
    "staging_italy_regioni_table_create= (\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS staging_italy_regioni\n",
    "(\n",
    "codice_regione INT,\n",
    "dt VARCHAR,\n",
    "deceduti INT,\n",
    "denominazione_regione VARCHAR,\n",
    "dimessi_guariti INT,\n",
    "lat FLOAT,\n",
    "long FLOAT,\n",
    "nuovi_positivi INT,\n",
    "ricoverati_con_sintomi INT,\n",
    "stato VARCHAR,\n",
    "tamponi INT,\n",
    "terapia_intensiva INT,\n",
    "totale_casi INT,\n",
    "totale_ospedalizzati INT,\n",
    "totale_positivi INT,\n",
    "variazione_totale_positivi INT)\n",
    "\"\"\")\n",
    "\n",
    "staging_germany_table_create = (\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS staging_germany\n",
    "(\n",
    "state VARCHAR,\n",
    "county VARCHAR,\n",
    "age_group VARCHAR,\n",
    "gender VARCHAR,\n",
    "dt VARCHAR,\n",
    "cases INT,\n",
    "deaths INT,\n",
    "recovered INT)\n",
    "\"\"\")\n",
    "\n",
    "# COPYING TO STAGING TABLES\n",
    "staging_italy_province_copy = (\"\"\"COPY staging_italy_province FROM '{}' iam_role '{}' FORMAT AS JSON 'auto'\"\"\").format(italy_province_staging_table_s3_path, DWH_IAM_ROLE_WITH_ARN)\n",
    "\n",
    "staging_italy_regioni_copy = (\"\"\"COPY staging_italy_regioni FROM '{}' iam_role '{}' FORMAT AS JSON 'auto'\"\"\").format(italy_regioni_staging_table_s3_path, DWH_IAM_ROLE_WITH_ARN)\n",
    "\n",
    "staging_germany_copy = (\"\"\"COPY staging_germany FROM '{}' iam_role '{}' FORMAT AS JSON 'auto'\"\"\").format(germany_staging_table_s3_path, DWH_IAM_ROLE_WITH_ARN)\n",
    "\n",
    "# QUERY LISTS\n",
    "create_table_queries = [staging_italy_province_table_create, staging_italy_regioni_table_create, staging_germany_table_create]\n",
    "drop_table_queries = [staging_italy_province_table_drop, staging_italy_regioni_table_drop, staging_germany_table_drop]\n",
    "copy_table_queries = [staging_germany_copy, staging_italy_province_copy, staging_italy_regioni_copy]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0aa92aba423d4b1d9e3930468d055eb9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def drop_tables(cur, conn):\n",
    "    \"\"\"\n",
    "    - drop table if exists in AWS Redshift\n",
    "    \"\"\"\n",
    "    for query in drop_table_queries:\n",
    "        cur.execute(query)\n",
    "        conn.commit()\n",
    "\n",
    "def create_tables(cur, conn):\n",
    "    \"\"\"\n",
    "    - Create table statements for AWS Redshift\n",
    "    \"\"\"\n",
    "    for query in create_table_queries:\n",
    "        cur.execute(query)\n",
    "        conn.commit()\n",
    "        \n",
    "def load_staging_tables(cur, conn):\n",
    "    \"\"\"\n",
    "    - Load files from s3 bucket into staging tables\n",
    "    \"\"\"\n",
    "    for query in copy_table_queries:\n",
    "        cur.execute(query)\n",
    "        conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b84a1e7a95748bb98b9e565ee728e5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "drop_tables(cur,conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf9f883a99ba4d1482b04e9221b805e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "create_tables(cur, conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28213b92a838403c9345778c85ada284",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "load_staging_tables(cur, conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Behavioral analysis of governments during the pandemic\n",
    "### Udacity - Data Engineering Capstone Project\n",
    "\n",
    "#### Project Summary\n",
    "\n",
    "This project aims to demonstrate how governments and authorities can induce the population to follow (right or wrong) ideas, specifically with regard to Coronavirus disease (COVID-19). The main reason to idealize this analysis is show how a lot of deaths could be avoid when the right (or better) person is in the right place. Analyzing results in these datasets in Spark or tables in Redshift, We can be sure that public politics applied the right way, it could save lifes.\n",
    "\n",
    "These two videos can explain how different was the treatment in the beginning of this crisis:\n",
    "\n",
    "* \"Italy doesn't stop, Milan doesn't stop\" - https://www.youtube.com/watch?v=kqQ0M42I_A0\n",
    "\n",
    "* \"Angela Merkel uses science background in coronavirus explainer\" - https://www.youtube.com/watch?v=22SQVZ4CeXA\n",
    "\n",
    "#### Gathering Data\n",
    "-- To perform this analysis, it was used the following datasets available in Kaggle:\n",
    "* https://www.kaggle.com/headsortails/covid19-tracking-germany - Germany Daily Updated Cases & Deaths - Augmented with geospatial & demographics info\n",
    "* https://www.kaggle.com/lorenzopagliaro01/coronavirus-italian-data - Evolution of Coronavirus in Italy with daily data from the italian government\n",
    "\n",
    "-- Format Types:\n",
    "* Germany file is available in CSV format with header\n",
    "* Italy files are available in JSON format\n",
    "\n",
    "-- Italy CSV official files is broken since there are some records without comma and quotes, to keep integrity of data I prefered to process Italy JSON files and Germany files in CSV format. Then, another limitation came out, that AWS Redshift only work with JSON file size under 4MB, for that reason, all JSON files (and Germany CSV files, just for padronization) was read by Spark and then write to S3 Bucket as JSON in small files, to be able to work with AWS Redshift.\n",
    "\n",
    "-- Technologies used to do this analysis:\n",
    "* AWS S3\n",
    "* AWS EMR (Apache Spark, Jupyter Notebook)\n",
    "* AWS Redshift\n",
    "* Apache Airflow\n",
    "* Jupyter Notebook (to process JSON files in small parts, and to analyze data after data pipeline process)\n",
    "\n",
    "#### Data Model\n",
    "\n",
    "First of all, all data start being processed by AWS EMR (Apache Spark and Jupyter Notebook inside the cluster), creating a notebook using graphical interface:\n",
    "\n",
    "![Creating Notebook in AWS EMR](emr_notebook.png)\n",
    "\n",
    "All original data files (from Kaggle), JSON and CSV files are in a S3 bucket, it can be updated by downloading new version of files using API or download button in Kaggle. These files in s3 bucket is read by Spark (EMR cluster using notebook commands), and save back to s3 as JSON in small parts than original file. AWS Redshift quota for single row size when loading by COPY is 4MB and it's not adjustable, that's why Spark is in this pipeline.\n",
    "\n",
    "![Spark Writing new file to S3](spark_writing.png)\n",
    "\n",
    "After you created a EMR cluster it is possible creating a notebook using graphical interface instead of inside the EMR cluster like image below.\n",
    "Next step is install and update the following packages in EMR cluster to get commands done without erros in Jupyter Notebook, and it's possible to config a bootstrap file to do that every time a new cluster is starting:\n",
    "\n",
    "```\n",
    "yum install -y python36 python36-devel python36-setuptools gcc python-setuptools python-devel postgresql-devel\n",
    "easy_install-3.6 pip\n",
    "pip3 install boto3 psycopg2\n",
    "pip install --upgrade pip\n",
    "```\n",
    "\n",
    "This project ingest using Apache Airflow in AWS Redshift 3 tables, a kind of \"fact table and two dimension tables\", these \"dimension tables\" contains data related to country like state id and names, city id and names, also, Coronavirus disease (COVID-19) cases count by city is also included in city table, and another table called time just keep all occurrence dates available in this analysis.\n",
    "\n",
    "![Data Model](data_model.png)\n",
    "\n",
    "#### Data Pipeline\n",
    "\n",
    "After start Airflow service, Redshift connection is configured before turn on DAG:\n",
    "\n",
    "![Airflow Redshift Connection](airflow_redshift_conn.png)\n",
    "\n",
    "Airflow DAG first step is run data quality task in staging tables that was populate by EMR/Spark, after that, time, city and covid_analysis tables are created and data is inserted, and finally, data quality task runs before end execution.\n",
    "\n",
    "![Airflow Tasks](airflow_tasks.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 2
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
